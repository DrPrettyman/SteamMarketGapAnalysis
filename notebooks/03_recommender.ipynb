{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Recommendation Engine\n",
    "\n",
    "This notebook develops and evaluates a hybrid recommendation engine:\n",
    "\n",
    "1. **Collaborative filtering** (ALS) on implicit feedback (playtime)\n",
    "2. **Content-based** (cosine similarity on game features) for cold-start\n",
    "3. **Hybrid blend** with confidence-weighted α\n",
    "\n",
    "## Why ALS over Neural Approaches?\n",
    "\n",
    "- Transparency and explainability — latent factors can be inspected\n",
    "- Well-suited for implicit feedback (playtime, not ratings)\n",
    "- Scales to our dataset size without GPU requirements\n",
    "- Follows the principle: avoid unnecessary complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from src.evaluation.metrics import evaluate_recommender\n",
    "from src.evaluation.validation import (\n",
    "    cold_start_split,\n",
    "    leave_one_out_split,\n",
    "    popularity_baseline,\n",
    ")\n",
    "from src.models.recommender import (\n",
    "    CollaborativeFilter,\n",
    "    ContentBasedFilter,\n",
    "    HybridRecommender,\n",
    ")\n",
    "from src.processing.features import build_game_features, build_interaction_matrix\n",
    "\n",
    "PROCESSED_DIR = Path(\"../data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "games = pd.read_json(PROCESSED_DIR / \"games.json\", lines=True)\n",
    "user_games = pd.read_csv(PROCESSED_DIR / \"user_games.csv\")\n",
    "\n",
    "print(f\"Games: {len(games):,} | User-game pairs: {len(user_games):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Interaction Matrix & Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction matrix (implicit feedback: log1p(playtime))\n",
    "interaction_matrix, user_ids, app_ids = build_interaction_matrix(user_games, min_playtime=0)\n",
    "print(f\"Interaction matrix: {interaction_matrix.shape}\")\n",
    "print(f\"Sparsity: {1 - interaction_matrix.nnz / (interaction_matrix.shape[0] * interaction_matrix.shape[1]):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based feature vectors\n",
    "feature_df, feature_meta = build_game_features(games)\n",
    "\n",
    "# Align feature matrix with app_ids from the interaction matrix\n",
    "app_id_to_idx = {aid: i for i, aid in enumerate(app_ids)}\n",
    "feature_df_aligned = feature_df[feature_df[\"app_id\"].isin(app_ids)].copy()\n",
    "feature_df_aligned[\"item_idx\"] = feature_df_aligned[\"app_id\"].map(app_id_to_idx)\n",
    "feature_df_aligned = feature_df_aligned.sort_values(\"item_idx\").reset_index(drop=True)\n",
    "\n",
    "numeric_cols = [c for c in feature_df_aligned.columns if c not in (\"app_id\", \"item_idx\")]\n",
    "feature_matrix = feature_df_aligned[numeric_cols].fillna(0).values\n",
    "print(f\"Feature matrix: {feature_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Using leave-one-out evaluation: for each user, hold out one game and try to recommend it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix, test_data = leave_one_out_split(interaction_matrix)\n",
    "print(f\"Train nnz: {train_matrix.nnz:,} | Test users: {len(test_data):,}\")\n",
    "\n",
    "# Cold-start split\n",
    "warm_test, cold_test = cold_start_split(test_data, interaction_matrix, threshold=100)\n",
    "print(f\"Warm test: {len(warm_test):,} | Cold test: {len(cold_test):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering\n",
    "cf = CollaborativeFilter(factors=64, regularization=0.01, iterations=15)\n",
    "cf.fit(train_matrix)\n",
    "\n",
    "# Content-based\n",
    "cb = ContentBasedFilter(feature_matrix)\n",
    "\n",
    "# Hybrid\n",
    "hybrid = HybridRecommender(cf, cb, interaction_threshold=100)\n",
    "hybrid.set_interaction_counts(interaction_matrix)\n",
    "\n",
    "print(\"All models trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Standard Metrics: P@K, Recall@K, NDCG@K\n",
    "\n",
    "We do **not** report accuracy — it's meaningless for implicit feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build revenue map for revenue-weighted metrics\n",
    "revenue_map = {}\n",
    "if \"estimated_revenue\" in games.columns:\n",
    "    for _, row in games.iterrows():\n",
    "        aid = row[\"app_id\"]\n",
    "        if aid in app_id_to_idx:\n",
    "            revenue_map[app_id_to_idx[aid]] = row.get(\"estimated_revenue\", 0)\n",
    "\n",
    "# Evaluate all test users\n",
    "print(\"=== All Users ===\")\n",
    "all_results = evaluate_recommender(hybrid, test_data, item_revenues=revenue_map, k_values=[5, 10, 20])\n",
    "for metric, value in sorted(all_results.items()):\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm vs. cold-start comparison\n",
    "print(\"\\n=== Warm Items (≥100 interactions) ===\")\n",
    "warm_results = evaluate_recommender(hybrid, warm_test, item_revenues=revenue_map)\n",
    "for metric, value in sorted(warm_results.items()):\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\n=== Cold-Start Items (<100 interactions) ===\")\n",
    "cold_results = evaluate_recommender(hybrid, cold_test, item_revenues=revenue_map)\n",
    "for metric, value in sorted(cold_results.items()):\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popularity Baseline Comparison\n",
    "\n",
    "Any useful recommender should beat a simple popularity baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_items = popularity_baseline(interaction_matrix, n=20)\n",
    "print(f\"Popularity baseline top-20 items: {pop_items}\")\n",
    "\n",
    "# Manual baseline evaluation\n",
    "from src.evaluation.metrics import precision_at_k, ndcg_at_k\n",
    "\n",
    "baseline_p10 = np.mean([\n",
    "    precision_at_k(pop_items, set(e[\"test_items\"]), 10)\n",
    "    for e in test_data\n",
    "])\n",
    "baseline_ndcg10 = np.mean([\n",
    "    ndcg_at_k(pop_items, set(e[\"test_items\"]), 10)\n",
    "    for e in test_data\n",
    "])\n",
    "print(f\"\\nPopularity baseline — P@10: {baseline_p10:.4f}, NDCG@10: {baseline_ndcg10:.4f}\")\n",
    "print(f\"Hybrid model      — P@10: {all_results.get('precision@10', 0):.4f}, NDCG@10: {all_results.get('ndcg@10', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "*To be filled after running with real data:*\n",
    "\n",
    "- For games with >100 interactions, hybrid achieves P@10 of X\n",
    "- For cold-start games (<100 interactions), content-based achieves P@10 of Y, vs. Z for popularity baseline\n",
    "- Revenue-weighted hit rate shows the model surfaces high-value games, not just popular free titles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
